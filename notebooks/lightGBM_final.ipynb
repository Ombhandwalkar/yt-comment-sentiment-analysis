{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4EQ40YgTN-H"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIfEN-g1Tgvx"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv('/content/reddit_preprocessing.csv')\n",
        "\n",
        "# Drop rows with NaN values in 'clean_comment'\n",
        "cleaned_dataset = dataset.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9cZ3TBRTmLC"
      },
      "outputs": [],
      "source": [
        "# Separate features and target\n",
        "X_cleaned = cleaned_dataset['clean_comment']\n",
        "y_cleaned = cleaned_dataset['category']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q32GE1IbTog4"
      },
      "outputs": [],
      "source": [
        "# Split the cleaned data into train and test sets (80-20 split)\n",
        "X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(X_cleaned, y_cleaned, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-4Fx-w6Tqw3"
      },
      "outputs": [],
      "source": [
        "# Apply TfidfVectorizer with trigram setting and max_features=1000\n",
        "tfidf_cleaned = TfidfVectorizer(ngram_range=(1, 3), max_features=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_Ft4yrATtCA"
      },
      "outputs": [],
      "source": [
        "# Fit the vectorizer on the training data and transform both train and test sets\n",
        "X_train_tfidf_cleaned = tfidf_cleaned.fit_transform(X_train_cleaned)\n",
        "X_test_tfidf_cleaned = tfidf_cleaned.transform(X_test_cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDNmrQ4DUUzh"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHvVmJHNT3dS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6686871e-3093-4088-cd2f-869421a7389c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f88ZFXmURpP"
      },
      "outputs": [],
      "source": [
        "# Function to optimize LightGBM hyperparameters\n",
        "def objective(trial):\n",
        "    # Define hyperparameters to be tuned\n",
        "    param = {\n",
        "        \"objective\": \"multiclass\",\n",
        "        \"num_class\": 3,  # Assuming 3 categories (-1, 0, 1)\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 1e-1),\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n",
        "        \"metric\": \"multi_logloss\",\n",
        "        \"is_unbalance\": True,\n",
        "        \"class_weight\": \"balanced\",\n",
        "    }\n",
        "\n",
        "    # Define the LightGBM model with the trial parameters\n",
        "    model = lgb.LGBMClassifier(**param)\n",
        "\n",
        "    # Perform cross-validation\n",
        "    scores = cross_val_score(model, X_train_tfidf_cleaned, y_train_cleaned, cv=3, scoring='accuracy')\n",
        "\n",
        "    # Return the average score across folds\n",
        "    return scores.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNrT1SQ-Uava"
      },
      "outputs": [],
      "source": [
        "# Create an Optuna study to optimize the hyperparameters\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RELIhmgZUdHh",
        "outputId": "bfd79917-20d9-492b-8627-3f444d708011"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'learning_rate': 0.08081298097796712, 'n_estimators': 367, 'max_depth': 20}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Extract the best hyperparameters\n",
        "best_params = study.best_params\n",
        "best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsUcrblLVGM7"
      },
      "outputs": [],
      "source": [
        "best_model = lgb.LGBMClassifier(\n",
        "\n",
        "    objective='multiclass',\n",
        "    num_class=3,\n",
        "    metric=\"multi_logloss\",\n",
        "    is_unbalance= True,\n",
        "    class_weight= \"balanced\",\n",
        "    reg_alpha= 0.1,  # L1 regularization\n",
        "    reg_lambda= 0.1,  # L2 regularization\n",
        "    learning_rate= 0.08,\n",
        "    max_depth= 20,\n",
        "    n_estimators=367\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X4WLnHEVGbQ"
      },
      "outputs": [],
      "source": [
        "# Fit the model on the resampled training data\n",
        "best_model.fit(X_train_tfidf_cleaned, y_train_cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the train set\n",
        "y_train_pred = best_model.predict(X_train_tfidf_cleaned)"
      ],
      "metadata": {
        "id": "ya2k-S4nvXM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy on the test set\n",
        "accuracy_train = accuracy_score(y_train_cleaned, y_train_pred)\n",
        "accuracy_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcJ5bqLHvgVN",
        "outputId": "8271ee98-b5c8-46ff-ade8-a4dfce188ab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9276143066589383"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate classification report\n",
        "report_train = classification_report(y_train_cleaned, y_train_pred)\n",
        "print(report_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSBhYE0HvsRk",
        "outputId": "3e28ab44-a425-479a-ad2a-bcdf8c8ef99a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.91      0.90      0.91      6601\n",
            "           0       0.88      0.98      0.93     10134\n",
            "           1       0.98      0.90      0.94     12594\n",
            "\n",
            "    accuracy                           0.93     29329\n",
            "   macro avg       0.93      0.93      0.92     29329\n",
            "weighted avg       0.93      0.93      0.93     29329\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5N1kk7LMVGnS"
      },
      "outputs": [],
      "source": [
        "# Predict on the test set\n",
        "y_pred = best_model.predict(X_test_tfidf_cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cz0bNLMCVGzG",
        "outputId": "56d00262-f8d4-491f-eb15-d18bc40fb3b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8658120823673804"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Calculate accuracy on the test set\n",
        "accuracy = accuracy_score(y_test_cleaned, y_pred)\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_joKbFHhVSbM",
        "outputId": "e7135add-c8ee-4ff9-accf-1c5ea366f541"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.81      0.78      0.79      1647\n",
            "           0       0.84      0.97      0.90      2510\n",
            "           1       0.92      0.83      0.87      3176\n",
            "\n",
            "    accuracy                           0.87      7333\n",
            "   macro avg       0.86      0.86      0.86      7333\n",
            "weighted avg       0.87      0.87      0.86      7333\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate classification report\n",
        "report = classification_report(y_test_cleaned, y_pred)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2KcG7SohWye",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42896dc2-644d-4ae8-bd65-5c190219ac98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Sentiment: 0, Confidence: 0.7993257111935146\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have pre-trained tfidf_vectorizer and lgbm_model loaded\n",
        "# tfidf_vectorizer: Your trained TF-IDF vectorizer\n",
        "# lgbm_model: Your trained LightGBM model\n",
        "\n",
        "# Function to clean and preprocess a YouTube comment (same as used during training)\n",
        "def preprocess_comment(comment):\n",
        "    # Lowercasing\n",
        "    comment = comment.lower()\n",
        "\n",
        "    # Remove special characters, URLs, punctuation, and extra spaces\n",
        "    comment = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', comment, flags=re.MULTILINE)  # Remove URLs\n",
        "    comment = re.sub(r'\\W', ' ', comment)  # Remove special characters\n",
        "    comment = re.sub(r'\\s+', ' ', comment).strip()  # Remove extra spaces and newlines\n",
        "\n",
        "    return comment\n",
        "\n",
        "# Prediction function\n",
        "def predict_sentiment(comment, tfidf_vectorizer, lgbm_model):\n",
        "    # Step 1: Preprocess the YouTube comment\n",
        "    cleaned_comment = preprocess_comment(comment)\n",
        "\n",
        "    # Step 2: Transform the comment using the trained TF-IDF vectorizer\n",
        "    comment_tfidf = tfidf_vectorizer.transform([cleaned_comment])\n",
        "\n",
        "    # Step 3: Use the trained LightGBM model to predict the sentiment\n",
        "    prediction = lgbm_model.predict(comment_tfidf)\n",
        "    prediction_proba = lgbm_model.predict_proba(comment_tfidf)\n",
        "\n",
        "    # Step 4: Get the predicted sentiment (label) and probability\n",
        "    sentiment_class = np.argmax(prediction_proba)\n",
        "    sentiment_proba = np.max(prediction_proba)\n",
        "\n",
        "    # Step 5: Return the sentiment label and confidence\n",
        "    return {\n",
        "        'sentiment_class': int(prediction[0]),  # -1, 0, or 1 depending on your labels\n",
        "        'confidence': sentiment_proba\n",
        "    }\n",
        "\n",
        "# Example usage:\n",
        "comment1 = \"I absolutely hate this video!\"\n",
        "comment2 = \"The explanations were confusing and the video quality was poor.\"\n",
        "comment3 = \"I didn’t learn anything useful. Really disappointed.\"\n",
        "comment4 = \"Wow, the explanation was so clear and helpful. Definitely subscribing!\"\n",
        "comment5 = \"This is the worst video I’ve seen on this topic, very misleading\"\n",
        "comment6 = \"Not much to say about this, just a standard video.\"\n",
        "comment7 = \"The video is okay, but I expected more depth in the content.\"\n",
        "comment8 = \"Superb content! Mazaa aa gaya dekh ke. Best video on this topic!\"\n",
        "comment9 = \"Poor video quality aur explanation bhi weak tha.\"\n",
        "comment10 = \"Yeh video theek tha, but I was expecting more depth.\"\n",
        "result = predict_sentiment(comment10, tfidf_cleaned, best_model)\n",
        "print(f\"Predicted Sentiment: {result['sentiment_class']}, Confidence: {result['confidence']}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gH05lzOed_5u"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}